{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as pl\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error\n",
    "from sklearn import tree, svm\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./ML/bez_trip_id_a_timestamp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calculated_delay</th>\n",
       "      <th>delay_stop_departure</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>speed</th>\n",
       "      <th>distance_traveled</th>\n",
       "      <th>trip_delay</th>\n",
       "      <th>section_last_delay</th>\n",
       "      <th>section_avg_delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>205</td>\n",
       "      <td>198</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>57.3</td>\n",
       "      <td>242</td>\n",
       "      <td>268</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-67</td>\n",
       "      <td>-22</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>36.6</td>\n",
       "      <td>90</td>\n",
       "      <td>125</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>27.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113</td>\n",
       "      <td>166</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>18.3</td>\n",
       "      <td>30</td>\n",
       "      <td>277</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255</td>\n",
       "      <td>239</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>20.9</td>\n",
       "      <td>8</td>\n",
       "      <td>356</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   calculated_delay  delay_stop_departure  hour  minute  day_of_week  speed  \\\n",
       "0               205                   198    23       5            4      7   \n",
       "1               -67                   -22    23       5            4     21   \n",
       "2                45                    43    23       5            4     23   \n",
       "3               113                   166    23       5            4     55   \n",
       "4               255                   239    23       5            4     33   \n",
       "\n",
       "   distance_traveled  trip_delay  section_last_delay  section_avg_delay  \n",
       "0               57.3         242                 268                214  \n",
       "1               36.6          90                 125                201  \n",
       "2               27.3           0                   7                 61  \n",
       "3               18.3          30                 277                 79  \n",
       "4               20.9           8                 356                305  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10863314 entries, 0 to 10863313\n",
      "Data columns (total 10 columns):\n",
      "calculated_delay        int64\n",
      "delay_stop_departure    int64\n",
      "hour                    int64\n",
      "minute                  int64\n",
      "day_of_week             int64\n",
      "speed                   int64\n",
      "distance_traveled       float64\n",
      "trip_delay              int64\n",
      "section_last_delay      int64\n",
      "section_avg_delay       int64\n",
      "dtypes: float64(1), int64(9)\n",
      "memory usage: 828.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAESCAYAAADtzi4UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAATwUlEQVR4nO3df9BmZX3f8fdHNqCtPwD30VoWXGhAs1KjuCJqmhDQuJCUbafEWdT6i2YnRhxbTR0MDrF0Ogna0akRpZtGCVRBoDXdjqQEf9I6gCzCIgtBVzDuog3LL9vGiYT47R/nWr334flx73Lv8zx7Pe/XzD17znWuc851zp797LnPOfd1UlVIkg58T1rsBkiSJsNAl6ROGOiS1AkDXZI6YaBLUicMdEnqxKIGepJPJLk/yR1j1P1wktva55tJHlmINkrSgSKL+Rx6kl8E/h9waVUdvxfzvQN4cVW9db81TpIOMIt6hl5V1wMPjZYl+QdJ/keSW5L8zyTPn2HWs4DLF6SRknSAWLHYDZjBJuA3q+pbSV4GfAw4ZffEJM8Fjga+uEjtk6QlaUkFepKnAq8Arkqyu/iQadU2AFdX1d8uZNskaalbUoHOcAnokap60Rx1NgBvX6D2SNIBY0k9tlhV/we4N8mvA2Tw87unt+vphwE3LFITJWnJWuzHFi9nCOfnJdmZ5Gzg9cDZSbYC24D1I7NsAK4ou4iUpMdZ1McWJUmTs6QuuUiS9t2i3RRduXJlrV69erFWL0kHpFtuueWBqpqaadqiBfrq1avZsmXLYq1ekg5ISf5itmlecpGkThjoktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4stf7QJWlBfPqm7+7zvK972VETbMnkeIYuSZ0w0CWpEwa6JHXCQJekThjoktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1Yt5AT/KJJPcnuWOW6UnykSTbk9ye5ITJN1OSNJ9xztAvAdbNMf004Nj22Qh8/Ik3S5K0t+YN9Kq6HnhojirrgUtrcCNwaJLnTKqBkqTxTOIa+hHAjpHxna3scZJsTLIlyZZdu3ZNYNWSpN0W9KZoVW2qqrVVtXZqamohVy1J3ZtEoN8HHDkyvqqVSZIW0CQCfTPwxva0y0nAD6rq+xNYriRpL8z7CroklwMnAyuT7AR+F/gZgKq6GLgGOB3YDvwQeMv+aqwkaXbzBnpVnTXP9ALePrEWSZL2ib8UlaROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekThjoktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SeqEgS5JnTDQJakTBrokdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0YK9CTrEtyd5LtSc6dYfpRSb6U5NYktyc5ffJNlSTNZd5AT3IQcBFwGrAGOCvJmmnV3gdcWVUvBjYAH5t0QyVJcxvnDP1EYHtV3VNVjwJXAOun1Sng6W34GcD3JtdESdI4xgn0I4AdI+M7W9mo9wNvSLITuAZ4x0wLSrIxyZYkW3bt2rUPzZUkzWZSN0XPAi6pqlXA6cBlSR637KraVFVrq2rt1NTUhFYtSYLxAv0+4MiR8VWtbNTZwJUAVXUD8GRg5SQaKEkazziBfjNwbJKjkxzMcNNz87Q63wVOBUjycwyB7jUVSVpA8wZ6VT0GnANcC9zF8DTLtiQXJDmjVXs38BtJtgKXA2+uqtpfjZYkPd6KcSpV1TUMNztHy84fGb4TeOVkmyZJ2hv+UlSSOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SeqEgS5JnTDQJakTBrokdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekThjoktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRNjBXqSdUnuTrI9ybmz1HltkjuTbEvy6ck2U5I0nxXzVUhyEHAR8GpgJ3Bzks1VdedInWOB9wKvrKqHkzxrfzVYkjSzcc7QTwS2V9U9VfUocAWwflqd3wAuqqqHAarq/sk2U5I0n3EC/Qhgx8j4zlY26jjguCRfTXJjknUzLSjJxiRbkmzZtWvXvrVYkjSjSd0UXQEcC5wMnAX8YZJDp1eqqk1Vtbaq1k5NTU1o1ZIkGC/Q7wOOHBlf1cpG7QQ2V9XfVNW9wDcZAl6StEDGCfSbgWOTHJ3kYGADsHlanT9hODsnyUqGSzD3TLCdkqR5zBvoVfUYcA5wLXAXcGVVbUtyQZIzWrVrgQeT3Al8CfjXVfXg/mq0JOnx5n1sEaCqrgGumVZ2/shwAe9qH0nSIvCXopLUCQNdkjphoEtSJwx0SeqEgS5JnTDQJakTBrokdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekThjoktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SeqEgS5JnRgr0JOsS3J3ku1Jzp2j3j9LUknWTq6JkqRxzBvoSQ4CLgJOA9YAZyVZM0O9pwHvBG6adCMlSfMb5wz9RGB7Vd1TVY8CVwDrZ6j3b4ELgb+eYPskSWMaJ9CPAHaMjO9sZT+R5ATgyKr63FwLSrIxyZYkW3bt2rXXjZUkze4J3xRN8iTgQ8C756tbVZuqam1VrZ2amnqiq5YkjRgn0O8DjhwZX9XKdnsacDzw5STfAU4CNntjVJIW1jiBfjNwbJKjkxwMbAA2755YVT+oqpVVtbqqVgM3AmdU1Zb90mJJ0ozmDfSqegw4B7gWuAu4sqq2JbkgyRn7u4GSpPGsGKdSVV0DXDOt7PxZ6p78xJslSdpb/lJUkjphoEtSJwx0SeqEgS5JnTDQJakTBrokdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekThjoktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SeqEgS5JnTDQJakTYwV6knVJ7k6yPcm5M0x/V5I7k9ye5AtJnjv5pkqS5jJvoCc5CLgIOA1YA5yVZM20arcCa6vqhcDVwAcm3VBJ0tzGOUM/EdheVfdU1aPAFcD60QpV9aWq+mEbvRFYNdlmSpLmM06gHwHsGBnf2cpmczbwp0+kUZKkvbdikgtL8gZgLfBLs0zfCGwEOOqooya5akla9sY5Q78POHJkfFUr20OSVwHnAWdU1Y9mWlBVbaqqtVW1dmpqal/aK0maxTiBfjNwbJKjkxwMbAA2j1ZI8mLgPzKE+f2Tb6YkaT7zBnpVPQacA1wL3AVcWVXbklyQ5IxW7YPAU4GrktyWZPMsi5Mk7SdjXUOvqmuAa6aVnT8y/KoJt0uStJf8pagkdcJAl6ROGOiS1AkDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHXCQJekThjoktQJA12SOmGgS1InDHRJ6oSBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdkjphoEtSJwx0SerEisVuwL5Yfe7n5q3znd//1QVoiZa7Y977Oe75vb6OtdXnfm7Z/PvZdP23+c6DP9zr+X7ns98Yu+6TAj8ueOepx/IfvvCtn5Tvj33sGbr0BPy4FrsFeiL2Jcz31u5jZDTM9xcDXZI6YaBLUicMdEnqhIEuSZ0w0CWpEwa6JHVirEBPsi7J3Um2Jzl3humHJPlMm35TktWTbqgkaW7zBnqSg4CLgNOANcBZSdZMq3Y28HBV/SzwYeDCSTdUkjS3cX4peiKwvaruAUhyBbAeuHOkznrg/W34auCjSVJVi/azi0/f9N1Zp73uZUctYEskaWFkvsxNciawrqr+RRv/58DLquqckTp3tDo72/i3W50Hpi1rI7CxjT4PuHtSGzIhK4EH5q3VN/fBwP3gPoCluQ+eW1VTM01Y0L5cqmoTsGkh17k3kmypqrWL3Y7F5D4YuB/cB3Dg7YNxboreBxw5Mr6qlc1YJ8kK4BnAg5NooCRpPOME+s3AsUmOTnIwsAHYPK3OZuBNbfhM4IuLef1ckpajeS+5VNVjSc4BrgUOAj5RVduSXABsqarNwB8BlyXZDjzEEPoHoiV7OWgBuQ8G7gf3ARxg+2Dem6KSpAODvxSVpE4Y6JLUia4DPcmLktyY5LYkW5Kc2MqT5COtq4Lbk5wwMs+bknyrfd40Uv6SJN9o83wkSVr54Umua/WvS3LYwm/p/JK8I8mfJ9mW5AMj5e9t23R3kteMlM/Y3UO7OX5TK/9Mu1F+wHT/kOTdSSrJyja+bI6FJB9sx8DtST6b5NCRacvqOBjHbNu+pFVVtx/gz4DT2vDpwJdHhv8UCHAScFMrPxy4p/15WBs+rE37WqubNu/u5X4AOLcNnwtcuNjbPcN++GXg88AhbfxZ7c81wFbgEOBo4NsMN74PasPHAAe3OmvaPFcCG9rwxcDb2vBvARe34Q3AZxZ7u2fYD0cy3Nz/C2DlcjsWgF8BVrThC3e3b7kdB2Puq1m3fSl/uj5DBwp4eht+BvC9NrweuLQGNwKHJnkO8Brguqp6qKoeBq4D1rVpT6+qG2v4274U+Ccjy/rjNvzHI+VLyduA36+qHwFU1f2tfD1wRVX9qKruBbYzdPXwk+4equpR4ApgfTsTPYWhewfYc3tH98PVwKm7z1yXkA8D72E4LnZbNsdCVf1ZVT3WRm9k+E0JLL/jYBwzbvsit2levQf6vwQ+mGQH8O+B97byI4AdI/V2trK5ynfOUA7w7Kr6fhv+38CzJ7kBE3Ic8I/aV+CvJHlpK9/b/fBM4JGRUBjdDz+Zp03/Qau/JCRZD9xXVVunTVpux8Jub2X4dgHL6DjYC7Nt+5K2oD/93x+SfB74ezNMOg84FfhXVfVfkryW4Xn5V+2vtlRVJVmU50Dn2Q8rGC4dnAS8FLgyyTEL2LwFMc8++B2GSw4LYrGOhbn2QVX9t1bnPOAx4FML2Tbtfwd8oFfVrAGd5FLgnW30KuA/teHZujO4Dzh5WvmXW/mqGeoD/GWS51TV99vX8ftZBPPsh7cB/7VdIvhakh8zdDo0V7cOM5U/yHBJYkU7+xqtv3tZO7NI3T/Mtg+S/EOGa8Nb27f/VcDXM9wk7+pYmOs4AEjyZuDXgFPb8QCdHQcTMk6XJ0vPYl/E358f4C7g5DZ8KnBLG/5V9rwR9rVWfjhwL8NNsMPa8OFt2vQbYae38g+y542wDyz2ds+wH34TuKANH8fwVTLAC9jzZtg9DDeDVrTho/npDaEXtPmvYs+bYb/Vht/OnjfDrlzs7Z5jf3yHn94UXTbHArCOodvrqWnly/I4mGdfzbrtS/mz6A3Yz38pvwDc0v4ybgJe0srD8NKObwPfANaOzPNWhptC24G3jJSvBe5o83yUn/7K9pnAF4BvMTxJcvhib/cM++Fg4D+39n8dOGVk2nltm+6mPa3Ryk8HvtmmnTdSfkwLtO3tH/XuJ2ee3Ma3t+nHLPZ2z7E/RgN92RwLbTt2ALe1z8XL+TgYY3/NuO1L+eNP/yWpE70/5SJJy4aBLkmdMNAlqRMGuiR1wkCXpE4Y6JLUCQNdS1aSNyf56D7Oe0mSM8dY/t/fy+WuTnLHXtT/cpI53xo/Th1pHAa6lrM3A3sV6NJSZqBrwSV5Y3vJwtYklyX5x60nyFuTfD7J43opTPLs9lKGre3ziulny0l+O8n7Z5j3/CQ3J7kjyab2UoszGX7x+akML0B5SntxxVeS3JLk2tYfy+4XWmxNspXhp+1zbdtTklyR5K4knwWeMjLtV5LckOTrSa5K8tQZ5v94hpexbEvyb1rZKUn+ZKTOq9uypT0Y6FpQSV4AvI+h+4GfZ+g87X8BJ1XVixn6nX7PDLN+BPhKm+cEYNterPajVfXSqjqeIWB/raquBrYAr6+qFzH0PvgHwJlV9RLgE8C/a/N/EnhHW/d83gb8sKp+Dvhd4CVtu1e27X5VVZ3Q1v2uGeY/r6rWAi8EfinJC4EvAc9PMtXqvKW1T9rDAd/bog44pwBXVdUDAFX1UOsN8TPtjPhgho6wZprvjW2evwV+kPFf8fbLSd4D/B2GTre2Af99Wp3nAccD17UeGQ8Cvp/hNW2HVtX1rd5lwGlzrOsXGf7zoapuT3J7Kz+J4c1AX23LPxi4YYb5X5tkI8O/zecwvCXn9iSXAW9I8kng5bR9IY0y0LUU/AHwoaranORk4P1jzvcYe37LfPL0CkmeDHyModOtHe2SzOPqMXTSta2qXj5t/kNnqLsvwvAGpLNmrZAcDfw28NKqejjJJSNt/STDf0J/zfAf4mMzL0XLmZdctNC+CPx6kmfC8GJlhj6zd/c1/aZZ5vsCw+UMkhyU5BnAXwLPSvLMJIcw9PM93e5AfKBdsx598uX/Ak9rw3cDU0le3tbxM0leUFWPAI8k+YVW7/XzbN/1wOvaMo5nuHQCwyvfXpnkZ9u0v5vkuGnzPh34K4ZvH89m5JtAVX2P4RWK72MId+lxDHQtqKraxnBt+ivtJuOHGM7Ir0pyC/DALLO+k+HSyTcYukReU1V/A1zA0E3rdcCfz7C+R4A/ZOju9lrg5pHJlwAXJ7mN4RLLmcCFrV23Aa9o9d4CXNTqzfd+zI8DT01yV2vbLa0duxieqrm8XYa5AXj+tLZuBW5t2/Fp4KvTlv0pYEdV3TVPG7RM2X2udIBoz+TfWlV/tNht0dJkoEsHgPbt5a+AV1fVjxa7PVqaDHRpHyR5DXDhtOJ7q+qfLkZ7JDDQJakb3hSVpE4Y6JLUCQNdkjphoEtSJ/4/y4kx4uY0BKIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = sns.distplot(df['calculated_delay'], bins=20, kde = False, rug=True)\n",
    "fig.get_figure().savefig('calculated_delay_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_2 = sns.distplot(df['delay_stop_departure'], bins=20, kde = False, rug=True)\n",
    "fig_2.get_figure().savefig('delay_stop_departure_distr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calculated_delay</th>\n",
       "      <th>delay_stop_departure</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>speed</th>\n",
       "      <th>distance_traveled</th>\n",
       "      <th>trip_delay</th>\n",
       "      <th>section_last_delay</th>\n",
       "      <th>section_avg_delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.086331e+07</td>\n",
       "      <td>1.086331e+07</td>\n",
       "      <td>1.086331e+07</td>\n",
       "      <td>1.086331e+07</td>\n",
       "      <td>1.086331e+07</td>\n",
       "      <td>1.086331e+07</td>\n",
       "      <td>1.086331e+07</td>\n",
       "      <td>1.086331e+07</td>\n",
       "      <td>1.086331e+07</td>\n",
       "      <td>1.086331e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-6.293476e+01</td>\n",
       "      <td>1.539429e+02</td>\n",
       "      <td>1.174717e+01</td>\n",
       "      <td>2.948051e+01</td>\n",
       "      <td>3.578772e+00</td>\n",
       "      <td>2.996750e+01</td>\n",
       "      <td>1.207330e+01</td>\n",
       "      <td>7.974908e+01</td>\n",
       "      <td>1.604717e+02</td>\n",
       "      <td>1.264131e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.338557e+03</td>\n",
       "      <td>2.095885e+02</td>\n",
       "      <td>5.059448e+00</td>\n",
       "      <td>1.729529e+01</td>\n",
       "      <td>1.795433e+00</td>\n",
       "      <td>2.276382e+01</td>\n",
       "      <td>1.133600e+01</td>\n",
       "      <td>1.105582e+02</td>\n",
       "      <td>2.114124e+02</td>\n",
       "      <td>1.448300e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-8.666300e+04</td>\n",
       "      <td>-6.000000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.570000e+02</td>\n",
       "      <td>-5.950000e+02</td>\n",
       "      <td>-4.210000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>3.600000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.300000e+01</td>\n",
       "      <td>2.900000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.300000e+01</td>\n",
       "      <td>9.300000e+01</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>8.800000e+00</td>\n",
       "      <td>4.200000e+01</td>\n",
       "      <td>1.010000e+02</td>\n",
       "      <td>8.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.130000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>4.400000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>1.730000e+01</td>\n",
       "      <td>1.110000e+02</td>\n",
       "      <td>2.180000e+02</td>\n",
       "      <td>1.730000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.522000e+03</td>\n",
       "      <td>2.400000e+03</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>5.900000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>3.040000e+02</td>\n",
       "      <td>6.830000e+01</td>\n",
       "      <td>2.311000e+03</td>\n",
       "      <td>2.398000e+03</td>\n",
       "      <td>2.384000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       calculated_delay  delay_stop_departure          hour        minute  \\\n",
       "count      1.086331e+07          1.086331e+07  1.086331e+07  1.086331e+07   \n",
       "mean      -6.293476e+01          1.539429e+02  1.174717e+01  2.948051e+01   \n",
       "std        4.338557e+03          2.095885e+02  5.059448e+00  1.729529e+01   \n",
       "min       -8.666300e+04         -6.000000e+02  0.000000e+00  0.000000e+00   \n",
       "25%        2.300000e+01          2.900000e+01  7.000000e+00  1.500000e+01   \n",
       "50%        9.300000e+01          9.300000e+01  1.200000e+01  2.900000e+01   \n",
       "75%        2.130000e+02          2.080000e+02  1.600000e+01  4.400000e+01   \n",
       "max        6.522000e+03          2.400000e+03  2.300000e+01  5.900000e+01   \n",
       "\n",
       "        day_of_week         speed  distance_traveled    trip_delay  \\\n",
       "count  1.086331e+07  1.086331e+07       1.086331e+07  1.086331e+07   \n",
       "mean   3.578772e+00  2.996750e+01       1.207330e+01  7.974908e+01   \n",
       "std    1.795433e+00  2.276382e+01       1.133600e+01  1.105582e+02   \n",
       "min    1.000000e+00 -1.000000e+00       0.000000e+00 -1.570000e+02   \n",
       "25%    2.000000e+00  1.100000e+01       3.600000e+00  3.000000e+00   \n",
       "50%    4.000000e+00  3.000000e+01       8.800000e+00  4.200000e+01   \n",
       "75%    5.000000e+00  4.500000e+01       1.730000e+01  1.110000e+02   \n",
       "max    7.000000e+00  3.040000e+02       6.830000e+01  2.311000e+03   \n",
       "\n",
       "       section_last_delay  section_avg_delay  \n",
       "count        1.086331e+07       1.086331e+07  \n",
       "mean         1.604717e+02       1.264131e+02  \n",
       "std          2.114124e+02       1.448300e+02  \n",
       "min         -5.950000e+02      -4.210000e+02  \n",
       "25%          3.300000e+01       2.900000e+01  \n",
       "50%          1.010000e+02       8.200000e+01  \n",
       "75%          2.180000e+02       1.730000e+02  \n",
       "max          2.398000e+03       2.384000e+03  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_outlier(df, 'calculated_delay')\n",
    "df = remove_outlier(df, 'delay_stop_departure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9809330 entries, 0 to 10863313\n",
      "Data columns (total 10 columns):\n",
      "calculated_delay        int64\n",
      "delay_stop_departure    int64\n",
      "hour                    int64\n",
      "minute                  int64\n",
      "day_of_week             int64\n",
      "speed                   int64\n",
      "distance_traveled       float64\n",
      "trip_delay              int64\n",
      "section_last_delay      int64\n",
      "section_avg_delay       int64\n",
      "dtypes: float64(1), int64(9)\n",
      "memory usage: 823.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calculated_delay</th>\n",
       "      <th>delay_stop_departure</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>speed</th>\n",
       "      <th>distance_traveled</th>\n",
       "      <th>trip_delay</th>\n",
       "      <th>section_last_delay</th>\n",
       "      <th>section_avg_delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.809330e+06</td>\n",
       "      <td>9.809330e+06</td>\n",
       "      <td>9.809330e+06</td>\n",
       "      <td>9.809330e+06</td>\n",
       "      <td>9.809330e+06</td>\n",
       "      <td>9.809330e+06</td>\n",
       "      <td>9.809330e+06</td>\n",
       "      <td>9.809330e+06</td>\n",
       "      <td>9.809330e+06</td>\n",
       "      <td>9.809330e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.017177e+02</td>\n",
       "      <td>1.053668e+02</td>\n",
       "      <td>1.169496e+01</td>\n",
       "      <td>2.952974e+01</td>\n",
       "      <td>3.606374e+00</td>\n",
       "      <td>3.002444e+01</td>\n",
       "      <td>1.133457e+01</td>\n",
       "      <td>6.337520e+01</td>\n",
       "      <td>1.362285e+02</td>\n",
       "      <td>1.018281e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.149295e+02</td>\n",
       "      <td>1.065491e+02</td>\n",
       "      <td>5.094760e+00</td>\n",
       "      <td>1.727480e+01</td>\n",
       "      <td>1.806250e+00</td>\n",
       "      <td>2.265729e+01</td>\n",
       "      <td>1.088599e+01</td>\n",
       "      <td>8.377568e+01</td>\n",
       "      <td>1.730849e+02</td>\n",
       "      <td>1.055651e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.610000e+02</td>\n",
       "      <td>-2.040000e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.570000e+02</td>\n",
       "      <td>-5.950000e+02</td>\n",
       "      <td>-2.930000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>3.300000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>2.500000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.800000e+01</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>3.100000e+01</td>\n",
       "      <td>8.100000e+00</td>\n",
       "      <td>3.500000e+01</td>\n",
       "      <td>9.100000e+01</td>\n",
       "      <td>7.300000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.700000e+02</td>\n",
       "      <td>1.680000e+02</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>1.610000e+01</td>\n",
       "      <td>9.300000e+01</td>\n",
       "      <td>1.910000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.970000e+02</td>\n",
       "      <td>4.100000e+02</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>5.900000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>3.040000e+02</td>\n",
       "      <td>6.830000e+01</td>\n",
       "      <td>1.914000e+03</td>\n",
       "      <td>2.398000e+03</td>\n",
       "      <td>1.837000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       calculated_delay  delay_stop_departure          hour        minute  \\\n",
       "count      9.809330e+06          9.809330e+06  9.809330e+06  9.809330e+06   \n",
       "mean       1.017177e+02          1.053668e+02  1.169496e+01  2.952974e+01   \n",
       "std        1.149295e+02          1.065491e+02  5.094760e+00  1.727480e+01   \n",
       "min       -2.610000e+02         -2.040000e+02  2.000000e+00  0.000000e+00   \n",
       "25%        1.800000e+01          2.400000e+01  7.000000e+00  1.500000e+01   \n",
       "50%        7.800000e+01          8.000000e+01  1.200000e+01  3.000000e+01   \n",
       "75%        1.700000e+02          1.680000e+02  1.600000e+01  4.500000e+01   \n",
       "max        4.970000e+02          4.100000e+02  2.300000e+01  5.900000e+01   \n",
       "\n",
       "        day_of_week         speed  distance_traveled    trip_delay  \\\n",
       "count  9.809330e+06  9.809330e+06       9.809330e+06  9.809330e+06   \n",
       "mean   3.606374e+00  3.002444e+01       1.133457e+01  6.337520e+01   \n",
       "std    1.806250e+00  2.265729e+01       1.088599e+01  8.377568e+01   \n",
       "min    1.000000e+00 -1.000000e+00       0.000000e+00 -1.570000e+02   \n",
       "25%    2.000000e+00  1.100000e+01       3.300000e+00  1.000000e+00   \n",
       "50%    4.000000e+00  3.100000e+01       8.100000e+00  3.500000e+01   \n",
       "75%    5.000000e+00  4.500000e+01       1.610000e+01  9.300000e+01   \n",
       "max    7.000000e+00  3.040000e+02       6.830000e+01  1.914000e+03   \n",
       "\n",
       "       section_last_delay  section_avg_delay  \n",
       "count        9.809330e+06       9.809330e+06  \n",
       "mean         1.362285e+02       1.018281e+02  \n",
       "std          1.730849e+02       1.055651e+02  \n",
       "min         -5.950000e+02      -2.930000e+02  \n",
       "25%          3.000000e+01       2.500000e+01  \n",
       "50%          9.100000e+01       7.300000e+01  \n",
       "75%          1.910000e+02       1.460000e+02  \n",
       "max          2.398000e+03       1.837000e+03  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = sns.distplot(df['calculated_delay'], bins=20, kde = False, rug=True)\n",
    "fig.get_figure().savefig('calculated_delay_distribution_after.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = sns.distplot(df['delay_stop_departure'], bins=20, kde = False, rug=True)\n",
    "fig.get_figure().savefig('delay_stop_departure_distr_after.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('delay_stop_departure', axis=1)\n",
    "y = df['delay_stop_departure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.475358828139065;10.0;1701.3463507188153\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeRegressor()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('{};{};{}'.format(mean_absolute_error(y_test, y_pred), median_absolute_error(y_test, y_pred), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n;mean_absolute_error;median_absolute_error;mean_squared_error\n",
      "1;52.396526864106946;42.695089630361196;4558.644288071018\n",
      "2;35.7475631746695;25.917842707802034;2660.6696724235717\n",
      "3;30.40864732177475;19.316845043814148;2178.719228136738\n",
      "4;27.61023521553049;17.18677955447295;1905.8517233488492\n",
      "5;26.211428079308305;15.343311899026496;1787.83263187458\n",
      "6;25.666405352653783;15.245629952216802;1723.7987794059197\n",
      "7;24.977727189176907;14.362835893486817;1666.8941220279228\n",
      "8;24.70741920204798;14.368955791218902;1627.8004700142947\n",
      "9;24.552840274089803;14.35472476472863;1595.658009791175\n",
      "10;24.338997288339492;14.351477250973147;1562.9201317099732\n",
      "11;24.091239824102825;14.194764546877224;1530.50731394911\n",
      "12;23.8515737023407;14.041595794031252;1497.015710157472\n",
      "13;23.63128489254406;13.982565379825658;1468.3844662292558\n",
      "14;23.371352305568195;13.810736026563376;1438.0966449066937\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "print('n;mean_absolute_error;median_absolute_error;mean_squared_error')\n",
    "for n in range(1,15):\n",
    "    clf = tree.DecisionTreeRegressor(max_depth=n)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('{};{};{};{}'.format(n,mean_absolute_error(y_test, y_pred), median_absolute_error(y_test, y_pred), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n;mean_absolute_error;median_absolute_error;mean_squared_error\n",
      "15;23.10098726090825;13.694183073955202;1407.144091884346\n",
      "16;22.867678386869027;13.544400144980074;1387.7614224077636\n",
      "17;22.630594434183116;13.370265151515127;1372.1519529059326\n",
      "18;22.400498567644245;13.189966433820194;1361.0857342405025\n",
      "19;22.19345792313317;12.968777103209021;1358.5476019278244\n",
      "20;22.02061832858087;12.753424657534254;1361.441434649544\n",
      "21;21.879117219782128;12.509895227008144;1371.518971262997\n",
      "22;21.78178338141676;12.282089552238801;1387.6490488745605\n",
      "23;21.72125966398723;12.025641025641029;1407.2997534267606\n",
      "24;21.71286440795633;11.833333333333336;1434.3987567886431\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "print('n;mean_absolute_error;median_absolute_error;mean_squared_error')\n",
    "for n in range(15,25):\n",
    "    clf = tree.DecisionTreeRegressor(max_depth=n)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('{};{};{};{}'.format(n,mean_absolute_error(y_test, y_pred), median_absolute_error(y_test, y_pred), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n;mean_absolute_error;median_absolute_error;mean_squared_error\n",
      "25;21.73569264072418;11.579999999999998;1464.5049559474323\n",
      "26;21.76019417511536;11.333333333333343;1489.8823829786566\n",
      "27;21.82547805785497;11.08923076923077;1519.1538238639578\n",
      "28;21.90693609958853;11.0;1549.7304556111103\n",
      "29;21.973350170777138;10.8840579710145;1570.5250293261638\n",
      "30;22.04506098967468;10.666666666666671;1592.157465801417\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2bbca4f5f67a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{};{};{};{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedian_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "print('n;mean_absolute_error;median_absolute_error;mean_squared_error')\n",
    "for n in range(25,40):\n",
    "    clf = tree.DecisionTreeRegressor(max_depth=n)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('{};{};{};{}'.format(n,mean_absolute_error(y_test, y_pred), median_absolute_error(y_test, y_pred), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('./ML/decision_tree_regressor.model', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n;mean_absolute_error;median_absolute_error;mean_squared_error\n",
      "1;17.175460250598157;5.0;1242.2022148811386\n",
      "3;19.23400893163279;10.666666666666657;1092.6976991689435\n",
      "5;20.151534406529287;11.799999999999997;1115.2781719240766\n",
      "7;20.84957564452849;12.428571428571427;1149.4555481404757\n",
      "9;21.35224837532794;12.888888888888886;1179.9872158306632\n",
      "11;21.68554438849174;13.181818181818182;1205.6431650903482\n",
      "13;21.923544923361405;13.30769230769232;1226.507918023245\n"
     ]
    }
   ],
   "source": [
    "print('n;mean_absolute_error;median_absolute_error;mean_squared_error')\n",
    "for n in range(1,16,2):\n",
    "    neigh = KNeighborsRegressor(n_neighbors=n)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    y_pred = neigh.predict(X_test)\n",
    "    print('{};{};{};{}'.format(n,mean_absolute_error(y_test, y_pred), median_absolute_error(y_test, y_pred), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n;mean_absolute_error;median_absolute_error;mean_squared_error\n",
      "1;17.17202321667229;5.0;1242.4057797525418\n",
      "11;21.67816708916179;13.181818181818182;1206.242772402136\n",
      "21;22.4722359684485;13.714285714285708;1285.823372923297\n",
      "31;22.816905501717372;13.83870967741936;1330.4399048457137\n",
      "41;23.023787638165178;13.902439024390247;1359.7999130769322\n",
      "51;23.170047756454675;13.941176470588236;1381.2149776947183\n",
      "61;23.278222967484957;13.967213114754102;1398.1908155668966\n",
      "71;23.36846746708215;14.0;1412.1806438407255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9fa7b512150c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mneigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{};{};{};{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedian_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/neighbors/_regression.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mchunked_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('n;mean_absolute_error;median_absolute_error;mean_squared_error')\n",
    "for n in range(1,100,10):\n",
    "    neigh = KNeighborsRegressor(n_neighbors=n)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    y_pred = neigh.predict(X_test)\n",
    "    print('{};{};{};{}'.format(n,mean_absolute_error(y_test, y_pred), median_absolute_error(y_test, y_pred), mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.467556803423015\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = linear_model.LinearRegression(normalize=True)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lin_reg = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.402678572770427\n",
      "18.431057519559467\n",
      "1991.71954245067\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_test, y_pred_lin_reg))\n",
    "print(median_absolute_error(y_test, y_pred_lin_reg))\n",
    "print(mean_squared_error(y_test, y_pred_lin_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.9, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = linear_model.Ridge(alpha=.9)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lin_reg = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.402678572794308\n",
      "18.431057516928362\n",
      "1991.7195424487845\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_test, y_pred_lin_reg))\n",
    "print(median_absolute_error(y_test, y_pred_lin_reg))\n",
    "print(mean_squared_error(y_test, y_pred_lin_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,\n",
       "              compute_score=False, copy_X=True, fit_intercept=True,\n",
       "              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, n_iter=300,\n",
       "              normalize=False, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = linear_model.BayesianRidge()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lin_reg = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.402679179387626\n",
      "18.431091591046126\n",
      "1991.7194911704557\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_test, y_pred_lin_reg))\n",
    "print(median_absolute_error(y_test, y_pred_lin_reg))\n",
    "print(mean_squared_error(y_test, y_pred_lin_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       10062.8815            7.37m\n",
      "         2        9002.8105            7.26m\n",
      "         3        8102.7757            7.14m\n",
      "         4        7348.1632            7.08m\n",
      "         5        6696.3374            7.01m\n",
      "         6        6144.5510            6.93m\n",
      "         7        5661.9811            6.86m\n",
      "         8        5250.2580            6.79m\n",
      "         9        4887.3888            6.74m\n",
      "        10        4576.1085            6.67m\n",
      "        20        2892.4490            5.91m\n",
      "        30        2312.3942            5.17m\n",
      "        40        2069.1676            4.44m\n",
      "        50        1953.7955            3.69m\n",
      "        60        1895.9279            2.95m\n",
      "        70        1865.7500            2.21m\n",
      "        80        1847.2643            1.47m\n",
      "        90        1832.8829           44.01s\n",
      "       100        1821.4785            0.00s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='ls', verbose=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(est, open('./ML/GradientBoostingRegressor.model', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grad_boost = est.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.87183879682102\n",
      "16.82312707151108\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_test, y_pred_grad_boost))\n",
    "print(median_absolute_error(y_test, y_pred_grad_boost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='square',\n",
       "                  n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ada_boost_reg = AdaBoostRegressor(loss='square')\n",
    "ada_boost_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ada_boost_reg, open('./ML/AdaBoostRegressor.model', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA boost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ada_boost = ada_boost_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.79193155762712\n",
      "63.54510503280004\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_test, y_pred_ada_boost))\n",
    "print(median_absolute_error(y_test, y_pred_ada_boost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 919.78161289\n",
      "Iteration 2, loss = 870.92511823\n",
      "Iteration 3, loss = 867.56004125\n",
      "Iteration 4, loss = 865.26721097\n",
      "Iteration 5, loss = 863.25831776\n",
      "Iteration 6, loss = 861.72080759\n",
      "Iteration 7, loss = 860.43953181\n",
      "Iteration 8, loss = 859.30390661\n",
      "Iteration 9, loss = 858.37279277\n",
      "Iteration 10, loss = 857.65232223\n",
      "Iteration 11, loss = 856.85088143\n",
      "Iteration 12, loss = 856.29471377\n",
      "Iteration 13, loss = 855.67558563\n",
      "Iteration 14, loss = 855.23809091\n",
      "Iteration 15, loss = 854.88635257\n",
      "Iteration 16, loss = 854.35874005\n",
      "Iteration 17, loss = 854.03183950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kerb/.local/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning:\n",
      "\n",
      "Training interrupted by user.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=5, learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=True,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 hidden lazer\n",
    "mlp_1h_layer = MLPRegressor(hidden_layer_sizes=(5), activation='relu', solver='adam', max_iter=200, verbose=True)\n",
    "mlp_1h_layer.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 896.88517342\n",
      "Iteration 2, loss = 836.10482156\n",
      "Iteration 3, loss = 831.04679108\n",
      "Iteration 4, loss = 827.32197407\n",
      "Iteration 5, loss = 824.68208082\n",
      "Iteration 6, loss = 823.06584850\n",
      "Iteration 7, loss = 821.09353486\n",
      "Iteration 8, loss = 818.31058406\n",
      "Iteration 9, loss = 814.71534199\n",
      "Iteration 10, loss = 811.23785218\n",
      "Iteration 11, loss = 809.05754552\n",
      "Iteration 12, loss = 808.23045436\n",
      "Iteration 13, loss = 807.01857493\n",
      "Iteration 14, loss = 806.29692549\n",
      "Iteration 15, loss = 805.58054579\n",
      "Iteration 16, loss = 805.04175906\n",
      "Iteration 17, loss = 804.47446108\n",
      "Iteration 18, loss = 804.15554719\n",
      "Iteration 19, loss = 803.27421042\n",
      "Iteration 20, loss = 802.66718796\n",
      "Iteration 21, loss = 802.23180810\n",
      "Iteration 22, loss = 801.63599507\n",
      "Iteration 23, loss = 800.91182386\n",
      "Iteration 24, loss = 800.35572000\n",
      "Iteration 25, loss = 799.98215714\n",
      "Iteration 26, loss = 799.30047550\n",
      "Iteration 27, loss = 798.93823852\n",
      "Iteration 28, loss = 798.67535908\n",
      "Iteration 29, loss = 798.20698181\n",
      "Iteration 30, loss = 798.02272926\n",
      "Iteration 31, loss = 797.76245907\n",
      "Iteration 32, loss = 797.35198547\n",
      "Iteration 33, loss = 796.98077867\n",
      "Iteration 34, loss = 797.06417005\n",
      "Iteration 35, loss = 796.91143930\n",
      "Iteration 36, loss = 796.56467643\n",
      "Iteration 37, loss = 796.58623440\n",
      "Iteration 38, loss = 796.50935752\n",
      "Iteration 39, loss = 796.20700707\n",
      "Iteration 40, loss = 796.33462886\n",
      "Iteration 41, loss = 795.96491215\n",
      "Iteration 42, loss = 795.93254081\n",
      "Iteration 43, loss = 795.72322228\n",
      "Iteration 44, loss = 795.62736142\n",
      "Iteration 45, loss = 795.47811310\n",
      "Iteration 46, loss = 795.40448461\n",
      "Iteration 47, loss = 795.17145361\n",
      "Iteration 48, loss = 795.11546943\n",
      "Iteration 49, loss = 795.06642671\n",
      "Iteration 50, loss = 795.05240949\n",
      "Iteration 51, loss = 794.89000882\n",
      "Iteration 52, loss = 795.02997070\n",
      "Iteration 53, loss = 794.73813935\n",
      "Iteration 54, loss = 794.64157576\n",
      "Iteration 55, loss = 794.64426215\n",
      "Iteration 56, loss = 794.56125554\n",
      "Iteration 57, loss = 794.60033176\n",
      "Iteration 58, loss = 794.62819570\n",
      "Iteration 59, loss = 794.39994826\n",
      "Iteration 60, loss = 794.35630304\n",
      "Iteration 61, loss = 794.09591446\n",
      "Iteration 62, loss = 794.22324741\n",
      "Iteration 63, loss = 794.19718089\n",
      "Iteration 64, loss = 794.15635349\n",
      "Iteration 65, loss = 794.21891539\n",
      "Iteration 66, loss = 794.17538776\n",
      "Iteration 67, loss = 794.03116760\n",
      "Iteration 68, loss = 794.01178895\n",
      "Iteration 69, loss = 793.95644615\n",
      "Iteration 70, loss = 793.92482220\n",
      "Iteration 71, loss = 793.75501993\n",
      "Iteration 72, loss = 793.84394707\n",
      "Iteration 73, loss = 793.68924836\n",
      "Iteration 74, loss = 793.71512578\n",
      "Iteration 75, loss = 793.77542321\n",
      "Iteration 76, loss = 793.48188669\n",
      "Iteration 77, loss = 793.65162982\n",
      "Iteration 78, loss = 793.64249467\n",
      "Iteration 79, loss = 793.60935161\n",
      "Iteration 80, loss = 793.48981540\n",
      "Iteration 81, loss = 793.43499472\n",
      "Iteration 82, loss = 793.28663860\n",
      "Iteration 83, loss = 793.19336388\n",
      "Iteration 84, loss = 793.29584804\n",
      "Iteration 85, loss = 793.15810098\n",
      "Iteration 86, loss = 793.11772017\n",
      "Iteration 87, loss = 793.16215319\n",
      "Iteration 88, loss = 793.17696007\n",
      "Iteration 89, loss = 792.62169965\n",
      "Iteration 90, loss = 792.42879374\n",
      "Iteration 91, loss = 792.37094201\n",
      "Iteration 92, loss = 792.13895194\n",
      "Iteration 93, loss = 792.12119753\n",
      "Iteration 94, loss = 792.14600239\n",
      "Iteration 95, loss = 792.01446487\n",
      "Iteration 96, loss = 792.04041339\n",
      "Iteration 97, loss = 791.95059833\n",
      "Iteration 98, loss = 791.96148857\n",
      "Iteration 99, loss = 791.99892199\n",
      "Iteration 100, loss = 791.83354064\n",
      "Iteration 101, loss = 791.94460657\n",
      "Iteration 102, loss = 791.86139454\n",
      "Iteration 103, loss = 791.67100666\n",
      "Iteration 104, loss = 791.77359486\n",
      "Iteration 105, loss = 791.72668206\n",
      "Iteration 106, loss = 791.67046240\n",
      "Iteration 107, loss = 791.63959868\n",
      "Iteration 108, loss = 791.79613087\n",
      "Iteration 109, loss = 791.55771783\n",
      "Iteration 110, loss = 791.60321788\n",
      "Iteration 111, loss = 791.52007912\n",
      "Iteration 112, loss = 791.58968216\n",
      "Iteration 113, loss = 791.46366830\n",
      "Iteration 114, loss = 791.54100164\n",
      "Iteration 115, loss = 791.40844659\n",
      "Iteration 116, loss = 791.53479363\n",
      "Iteration 117, loss = 791.50169799\n",
      "Iteration 118, loss = 791.44526121\n",
      "Iteration 119, loss = 791.38344756\n",
      "Iteration 120, loss = 791.49743172\n",
      "Iteration 121, loss = 791.44167247\n",
      "Iteration 122, loss = 791.46460634\n",
      "Iteration 123, loss = 791.39786477\n",
      "Iteration 124, loss = 791.34557764\n",
      "Iteration 125, loss = 791.31667607\n",
      "Iteration 126, loss = 791.30140390\n",
      "Iteration 127, loss = 791.34927908\n",
      "Iteration 128, loss = 791.29574223\n",
      "Iteration 129, loss = 791.32703252\n",
      "Iteration 130, loss = 791.23685402\n",
      "Iteration 131, loss = 791.21244983\n",
      "Iteration 132, loss = 791.24309394\n",
      "Iteration 133, loss = 791.10691993\n",
      "Iteration 134, loss = 791.14454293\n",
      "Iteration 135, loss = 791.19111904\n",
      "Iteration 136, loss = 791.14738659\n",
      "Iteration 137, loss = 791.07701509\n",
      "Iteration 138, loss = 791.09171268\n",
      "Iteration 139, loss = 791.18585656\n",
      "Iteration 140, loss = 791.01724448\n",
      "Iteration 141, loss = 791.09843617\n",
      "Iteration 142, loss = 791.05182544\n",
      "Iteration 143, loss = 790.96961760\n",
      "Iteration 144, loss = 790.96820521\n",
      "Iteration 145, loss = 790.98527721\n",
      "Iteration 146, loss = 790.90460331\n",
      "Iteration 147, loss = 790.98129243\n",
      "Iteration 148, loss = 790.98181891\n",
      "Iteration 149, loss = 791.00468352\n",
      "Iteration 150, loss = 790.82184380\n",
      "Iteration 151, loss = 790.97466829\n",
      "Iteration 152, loss = 790.92303141\n",
      "Iteration 153, loss = 790.85803985\n",
      "Iteration 154, loss = 790.86099042\n",
      "Iteration 155, loss = 790.91571683\n",
      "Iteration 156, loss = 790.94660332\n",
      "Iteration 157, loss = 790.88759364\n",
      "Iteration 158, loss = 790.91559220\n",
      "Iteration 159, loss = 790.83869181\n",
      "Iteration 160, loss = 790.80468214\n",
      "Iteration 161, loss = 790.78569361\n",
      "Iteration 162, loss = 790.78133938\n",
      "Iteration 163, loss = 790.82651293\n",
      "Iteration 164, loss = 790.65497728\n",
      "Iteration 165, loss = 790.63956016\n",
      "Iteration 166, loss = 790.54689695\n",
      "Iteration 167, loss = 790.72158579\n",
      "Iteration 168, loss = 790.64214165\n",
      "Iteration 169, loss = 790.64353506\n",
      "Iteration 170, loss = 790.70205803\n",
      "Iteration 171, loss = 790.55738730\n",
      "Iteration 172, loss = 790.65369207\n",
      "Iteration 173, loss = 790.62938545\n",
      "Iteration 174, loss = 790.55987048\n",
      "Iteration 175, loss = 790.50892238\n",
      "Iteration 176, loss = 790.52157733\n",
      "Iteration 177, loss = 790.53273425\n",
      "Iteration 178, loss = 790.42736899\n",
      "Iteration 179, loss = 790.41376627\n",
      "Iteration 180, loss = 790.51313972\n",
      "Iteration 181, loss = 790.39823864\n",
      "Iteration 182, loss = 790.31234188\n",
      "Iteration 183, loss = 790.45133190\n",
      "Iteration 184, loss = 790.38380630\n",
      "Iteration 185, loss = 790.40594095\n",
      "Iteration 186, loss = 790.37278794\n",
      "Iteration 187, loss = 790.29076322\n",
      "Iteration 188, loss = 790.41165987\n",
      "Iteration 189, loss = 790.41918640\n",
      "Iteration 190, loss = 790.25135740\n",
      "Iteration 191, loss = 790.39976741\n",
      "Iteration 192, loss = 790.31990365\n",
      "Iteration 193, loss = 790.29542977\n",
      "Iteration 194, loss = 790.34658028\n",
      "Iteration 195, loss = 790.22865714\n",
      "Iteration 196, loss = 790.24819199\n",
      "Iteration 197, loss = 790.32732806\n",
      "Iteration 198, loss = 790.23164836\n",
      "Iteration 199, loss = 790.09093266\n",
      "Iteration 200, loss = 790.16302937\n",
      "Iteration 201, loss = 790.25914776\n",
      "Iteration 202, loss = 790.32252736\n",
      "Iteration 203, loss = 790.04339354\n",
      "Iteration 204, loss = 790.09173924\n",
      "Iteration 205, loss = 790.12861801\n",
      "Iteration 206, loss = 790.13481915\n",
      "Iteration 207, loss = 790.20906415\n",
      "Iteration 208, loss = 790.07261009\n",
      "Iteration 209, loss = 790.11494492\n",
      "Iteration 210, loss = 790.09298314\n",
      "Iteration 211, loss = 790.16257311\n",
      "Iteration 212, loss = 790.16292270\n",
      "Iteration 213, loss = 790.11778425\n",
      "Iteration 214, loss = 790.12933010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(10, 5), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=True,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 hidden layers\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10,5), activation='relu', solver='adam', max_iter=100, verbose=True)\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.60833591371755\n",
      "14.488412114337557\n",
      "1576.2963050096637\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_test, y_pred_nn))\n",
    "print(median_absolute_error(y_test, y_pred_nn))\n",
    "print(mean_squared_error(y_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 920.21067591\n",
      "Iteration 2, loss = 827.74133941\n",
      "Iteration 3, loss = 824.38678405\n",
      "Iteration 4, loss = 822.07191846\n",
      "Iteration 5, loss = 819.36172214\n",
      "Iteration 6, loss = 816.46997711\n",
      "Iteration 7, loss = 814.33182539\n",
      "Iteration 8, loss = 812.46412094\n",
      "Iteration 9, loss = 810.24274241\n",
      "Iteration 10, loss = 807.24193407\n",
      "Iteration 11, loss = 803.47286911\n",
      "Iteration 12, loss = 800.28122849\n",
      "Iteration 13, loss = 798.77490605\n",
      "Iteration 14, loss = 797.12941475\n",
      "Iteration 15, loss = 796.30681558\n",
      "Iteration 16, loss = 795.81158228\n",
      "Iteration 17, loss = 795.20300098\n",
      "Iteration 18, loss = 794.74527786\n",
      "Iteration 19, loss = 794.57654325\n",
      "Iteration 20, loss = 794.46259659\n",
      "Iteration 21, loss = 794.00320044\n",
      "Iteration 22, loss = 794.10357832\n",
      "Iteration 23, loss = 793.18683994\n",
      "Iteration 24, loss = 793.64557443\n",
      "Iteration 25, loss = 793.33717031\n",
      "Iteration 26, loss = 793.47438018\n",
      "Iteration 27, loss = 792.97615091\n",
      "Iteration 28, loss = 793.28506471\n",
      "Iteration 29, loss = 792.77714313\n",
      "Iteration 30, loss = 792.44862837\n",
      "Iteration 31, loss = 792.35032538\n",
      "Iteration 32, loss = 792.12052404\n",
      "Iteration 33, loss = 791.68886580\n",
      "Iteration 34, loss = 791.27700296\n",
      "Iteration 35, loss = 791.27882657\n",
      "Iteration 36, loss = 790.95642305\n",
      "Iteration 37, loss = 791.10449757\n",
      "Iteration 38, loss = 790.91172766\n",
      "Iteration 39, loss = 790.55447316\n",
      "Iteration 40, loss = 790.42733786\n",
      "Iteration 41, loss = 790.40667205\n",
      "Iteration 42, loss = 790.33901979\n",
      "Iteration 43, loss = 790.25983876\n",
      "Iteration 44, loss = 790.19289153\n",
      "Iteration 45, loss = 789.96579790\n",
      "Iteration 46, loss = 790.07541388\n",
      "Iteration 47, loss = 790.29497256\n",
      "Iteration 48, loss = 789.96329932\n",
      "Iteration 49, loss = 789.93263236\n",
      "Iteration 50, loss = 789.90614382\n",
      "Iteration 51, loss = 789.81371487\n",
      "Iteration 52, loss = 789.58290038\n",
      "Iteration 53, loss = 789.79368301\n",
      "Iteration 54, loss = 789.59648697\n",
      "Iteration 55, loss = 789.69224487\n",
      "Iteration 56, loss = 789.53704814\n",
      "Iteration 57, loss = 789.38149872\n",
      "Iteration 58, loss = 789.21591587\n",
      "Iteration 59, loss = 789.17235820\n",
      "Iteration 60, loss = 788.90501604\n",
      "Iteration 61, loss = 789.13578369\n",
      "Iteration 62, loss = 788.98760572\n",
      "Iteration 63, loss = 788.76647108\n",
      "Iteration 64, loss = 789.17133303\n",
      "Iteration 65, loss = 788.84021717\n",
      "Iteration 66, loss = 788.75004692\n",
      "Iteration 67, loss = 788.95185740\n",
      "Iteration 68, loss = 789.06156078\n",
      "Iteration 69, loss = 788.81863006\n",
      "Iteration 70, loss = 788.74507987\n",
      "Iteration 71, loss = 788.55977087\n",
      "Iteration 72, loss = 788.73759550\n",
      "Iteration 73, loss = 788.51905552\n",
      "Iteration 74, loss = 788.37124375\n",
      "Iteration 75, loss = 788.45687545\n",
      "Iteration 76, loss = 788.54283198\n",
      "Iteration 77, loss = 788.45923623\n",
      "Iteration 78, loss = 788.37368541\n",
      "Iteration 79, loss = 788.24734051\n",
      "Iteration 80, loss = 788.42565761\n",
      "Iteration 81, loss = 788.24354724\n",
      "Iteration 82, loss = 788.27153585\n",
      "Iteration 83, loss = 788.25658260\n",
      "Iteration 84, loss = 788.04402606\n",
      "Iteration 85, loss = 788.18881640\n",
      "Iteration 86, loss = 788.22038531\n",
      "Iteration 87, loss = 788.02295390\n",
      "Iteration 88, loss = 788.01227936\n",
      "Iteration 89, loss = 787.93455291\n",
      "Iteration 90, loss = 787.89953153\n",
      "Iteration 91, loss = 788.11378031\n",
      "Iteration 92, loss = 787.99150194\n",
      "Iteration 93, loss = 787.84405868\n",
      "Iteration 94, loss = 787.95737914\n",
      "Iteration 95, loss = 787.88332062\n",
      "Iteration 96, loss = 787.68695231\n",
      "Iteration 97, loss = 787.80511130\n",
      "Iteration 98, loss = 787.81762390\n",
      "Iteration 99, loss = 787.63518556\n",
      "Iteration 100, loss = 787.86018463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kerb/.local/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(10, 5, 3), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=100,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=True,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 hidden layers\n",
    "mlp_3h_layers = MLPRegressor(hidden_layer_sizes=(10, 5, 3), activation='relu', solver='adam', max_iter=100, verbose=True)\n",
    "mlp_3h_layers.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn_3h = mlp_3h_layers.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.32057371249346\n",
      "14.200736453773903\n",
      "1565.8072169370423\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_test, y_pred_nn_3h))\n",
    "print(median_absolute_error(y_test, y_pred_nn_3h))\n",
    "print(mean_squared_error(y_test, y_pred_nn_3h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 876.31221747\n",
      "Iteration 2, loss = 829.62562835\n",
      "Iteration 3, loss = 826.05174127\n",
      "Iteration 4, loss = 823.57759986\n",
      "Iteration 5, loss = 820.81208830\n",
      "Iteration 6, loss = 816.71443739\n",
      "Iteration 7, loss = 812.31290002\n",
      "Iteration 8, loss = 810.05878490\n",
      "Iteration 9, loss = 808.76323522\n",
      "Iteration 10, loss = 806.61506179\n",
      "Iteration 11, loss = 804.95220288\n",
      "Iteration 12, loss = 804.15665898\n",
      "Iteration 13, loss = 802.93698999\n",
      "Iteration 14, loss = 801.26855102\n",
      "Iteration 15, loss = 800.06712119\n",
      "Iteration 16, loss = 799.03707329\n",
      "Iteration 17, loss = 798.52100451\n",
      "Iteration 18, loss = 798.06156829\n",
      "Iteration 19, loss = 797.62290700\n",
      "Iteration 20, loss = 797.37693031\n",
      "Iteration 21, loss = 796.99568845\n",
      "Iteration 22, loss = 796.76764563\n",
      "Iteration 23, loss = 796.36266208\n",
      "Iteration 24, loss = 796.09035408\n",
      "Iteration 25, loss = 795.86312140\n",
      "Iteration 26, loss = 795.60023048\n",
      "Iteration 27, loss = 795.28933543\n",
      "Iteration 28, loss = 794.87308496\n",
      "Iteration 29, loss = 794.91506874\n",
      "Iteration 30, loss = 794.49722035\n",
      "Iteration 31, loss = 794.37560569\n",
      "Iteration 32, loss = 794.19073565\n",
      "Iteration 33, loss = 793.79071626\n",
      "Iteration 34, loss = 793.69026982\n",
      "Iteration 35, loss = 793.48562591\n",
      "Iteration 36, loss = 793.40485417\n",
      "Iteration 37, loss = 793.00853169\n",
      "Iteration 38, loss = 792.95973810\n",
      "Iteration 39, loss = 792.82541405\n",
      "Iteration 40, loss = 792.66429927\n",
      "Iteration 41, loss = 792.15784279\n",
      "Iteration 42, loss = 791.89645778\n",
      "Iteration 43, loss = 791.32780626\n",
      "Iteration 44, loss = 791.32581378\n",
      "Iteration 45, loss = 791.20320382\n",
      "Iteration 46, loss = 790.86399194\n",
      "Iteration 47, loss = 790.62933120\n",
      "Iteration 48, loss = 790.67053087\n",
      "Iteration 49, loss = 790.28463447\n",
      "Iteration 50, loss = 790.30068631\n",
      "Iteration 51, loss = 789.94934643\n",
      "Iteration 52, loss = 789.95373563\n",
      "Iteration 53, loss = 789.78457753\n",
      "Iteration 54, loss = 789.68755208\n",
      "Iteration 55, loss = 789.57383146\n",
      "Iteration 56, loss = 789.48526836\n",
      "Iteration 57, loss = 789.22705770\n",
      "Iteration 58, loss = 789.33473986\n",
      "Iteration 59, loss = 788.99222908\n",
      "Iteration 60, loss = 788.91033495\n",
      "Iteration 61, loss = 788.99601612\n",
      "Iteration 62, loss = 788.61645173\n",
      "Iteration 63, loss = 788.60278679\n",
      "Iteration 64, loss = 788.57142040\n",
      "Iteration 65, loss = 788.46017860\n",
      "Iteration 66, loss = 788.22207447\n",
      "Iteration 67, loss = 788.42106719\n",
      "Iteration 68, loss = 787.99318011\n",
      "Iteration 69, loss = 788.03945514\n",
      "Iteration 70, loss = 787.94993631\n",
      "Iteration 71, loss = 787.97672595\n",
      "Iteration 72, loss = 787.85702838\n",
      "Iteration 73, loss = 787.78279461\n",
      "Iteration 74, loss = 787.68467421\n",
      "Iteration 75, loss = 787.67097169\n",
      "Iteration 76, loss = 787.55593844\n",
      "Iteration 77, loss = 787.40168624\n",
      "Iteration 78, loss = 787.33529655\n",
      "Iteration 79, loss = 787.15583394\n",
      "Iteration 80, loss = 787.15344720\n",
      "Iteration 81, loss = 787.02529265\n",
      "Iteration 82, loss = 786.99528955\n",
      "Iteration 83, loss = 786.67950351\n",
      "Iteration 84, loss = 786.52902473\n",
      "Iteration 85, loss = 786.44435624\n",
      "Iteration 86, loss = 786.22221410\n",
      "Iteration 87, loss = 786.16697148\n",
      "Iteration 88, loss = 786.13499531\n",
      "Iteration 89, loss = 785.65703396\n",
      "Iteration 90, loss = 785.77106921\n",
      "Iteration 91, loss = 785.65434503\n",
      "Iteration 92, loss = 785.52903186\n",
      "Iteration 93, loss = 785.37355208\n",
      "Iteration 94, loss = 785.28188944\n",
      "Iteration 95, loss = 785.19045769\n",
      "Iteration 96, loss = 785.07876316\n",
      "Iteration 97, loss = 785.21579930\n",
      "Iteration 98, loss = 785.13525952\n",
      "Iteration 99, loss = 784.82405940\n",
      "Iteration 100, loss = 785.06404470\n",
      "Iteration 101, loss = 784.77890982\n",
      "Iteration 102, loss = 785.00015105\n",
      "Iteration 103, loss = 784.85679186\n",
      "Iteration 104, loss = 784.59995627\n",
      "Iteration 105, loss = 784.57785276\n",
      "Iteration 106, loss = 784.59227479\n",
      "Iteration 107, loss = 784.46139492\n",
      "Iteration 108, loss = 784.48609350\n",
      "Iteration 109, loss = 784.35200697\n",
      "Iteration 110, loss = 784.46188806\n",
      "Iteration 111, loss = 784.21831080\n",
      "Iteration 112, loss = 784.15485092\n",
      "Iteration 113, loss = 784.05112333\n",
      "Iteration 114, loss = 784.15309300\n",
      "Iteration 115, loss = 783.92633094\n",
      "Iteration 116, loss = 783.85305509\n",
      "Iteration 117, loss = 783.56219694\n",
      "Iteration 118, loss = 783.79928611\n",
      "Iteration 119, loss = 783.38488905\n",
      "Iteration 120, loss = 783.59320541\n",
      "Iteration 121, loss = 783.63698692\n",
      "Iteration 122, loss = 783.57808457\n",
      "Iteration 123, loss = 783.42384351\n",
      "Iteration 124, loss = 783.42428754\n",
      "Iteration 125, loss = 783.35616848\n",
      "Iteration 126, loss = 783.21184146\n",
      "Iteration 127, loss = 783.21841915\n",
      "Iteration 128, loss = 783.28686468\n",
      "Iteration 129, loss = 783.17668021\n",
      "Iteration 130, loss = 783.09245224\n",
      "Iteration 131, loss = 783.08244156\n",
      "Iteration 132, loss = 783.08812635\n",
      "Iteration 133, loss = 782.93324822\n",
      "Iteration 134, loss = 783.14370025\n",
      "Iteration 135, loss = 783.07317097\n",
      "Iteration 136, loss = 783.02440319\n",
      "Iteration 137, loss = 782.91723583\n",
      "Iteration 138, loss = 782.99781545\n",
      "Iteration 139, loss = 782.85576577\n",
      "Iteration 140, loss = 782.74463277\n",
      "Iteration 141, loss = 782.88213289\n",
      "Iteration 142, loss = 782.70498062\n",
      "Iteration 143, loss = 782.86325583\n",
      "Iteration 144, loss = 782.72232040\n",
      "Iteration 145, loss = 782.78420704\n",
      "Iteration 146, loss = 782.65957595\n",
      "Iteration 147, loss = 782.60203391\n",
      "Iteration 148, loss = 782.63042200\n",
      "Iteration 149, loss = 782.59915366\n",
      "Iteration 150, loss = 782.60527467\n",
      "Iteration 151, loss = 782.59936743\n",
      "Iteration 152, loss = 782.54396642\n",
      "Iteration 153, loss = 782.52000957\n",
      "Iteration 154, loss = 782.42485100\n",
      "Iteration 155, loss = 782.28033987\n",
      "Iteration 156, loss = 782.43217990\n",
      "Iteration 157, loss = 782.16433240\n",
      "Iteration 158, loss = 782.14109275\n",
      "Iteration 159, loss = 782.32315115\n",
      "Iteration 160, loss = 782.05735745\n",
      "Iteration 161, loss = 782.27233752\n",
      "Iteration 162, loss = 782.03803507\n",
      "Iteration 163, loss = 782.18044296\n",
      "Iteration 164, loss = 782.02665012\n",
      "Iteration 165, loss = 782.18825328\n",
      "Iteration 166, loss = 782.10580631\n",
      "Iteration 167, loss = 781.95888975\n",
      "Iteration 168, loss = 782.00946545\n",
      "Iteration 169, loss = 782.04506779\n",
      "Iteration 170, loss = 781.90280725\n",
      "Iteration 171, loss = 781.98409288\n",
      "Iteration 172, loss = 781.83488238\n",
      "Iteration 173, loss = 781.94279092\n",
      "Iteration 174, loss = 781.99062424\n",
      "Iteration 175, loss = 781.74467848\n",
      "Iteration 176, loss = 781.79601665\n",
      "Iteration 177, loss = 782.11283453\n",
      "Iteration 178, loss = 781.89630190\n",
      "Iteration 179, loss = 781.73812802\n",
      "Iteration 180, loss = 781.69674811\n",
      "Iteration 181, loss = 781.95334410\n",
      "Iteration 182, loss = 781.93366261\n",
      "Iteration 183, loss = 781.79232071\n",
      "Iteration 184, loss = 781.69033128\n",
      "Iteration 185, loss = 781.77664193\n",
      "Iteration 186, loss = 781.66774751\n",
      "Iteration 187, loss = 781.68882604\n",
      "Iteration 188, loss = 781.73063899\n",
      "Iteration 189, loss = 781.72810151\n",
      "Iteration 190, loss = 781.63486732\n",
      "Iteration 191, loss = 781.58609703\n",
      "Iteration 192, loss = 781.80600796\n",
      "Iteration 193, loss = 781.60004460\n",
      "Iteration 194, loss = 781.54459129\n",
      "Iteration 195, loss = 781.72828095\n",
      "Iteration 196, loss = 781.51577130\n",
      "Iteration 197, loss = 781.57521345\n",
      "Iteration 198, loss = 781.78087646\n",
      "Iteration 199, loss = 781.32609156\n",
      "Iteration 200, loss = 781.43342517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kerb/.local/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(10, 7, 5, 3), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=True,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 hidden layers\n",
    "mlp_4h_layers = MLPRegressor(hidden_layer_sizes=(10, 7, 5, 3), activation='relu', solver='adam', max_iter=200, verbose=True)\n",
    "mlp_4h_layers.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn_4h = mlp_4h_layers.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.93060758250108\n",
      "15.234262345823954\n",
      "1565.080445333145\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_test, y_pred_nn_4h))\n",
    "print(median_absolute_error(y_test, y_pred_nn_4h))\n",
    "print(mean_squared_error(y_test, y_pred_nn_4h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "clf_svr = svm.SVR(verbose=True)\n",
    "clf_svr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svr = clf_svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_error(y_test, y_pred_svr))\n",
    "print(median_absolute_error(y_test, y_pred_svr))\n",
    "print(mean_squared_error(y_test, y_pred_svr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
